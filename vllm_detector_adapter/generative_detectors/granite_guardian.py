# Standard
from http import HTTPStatus
from typing import Dict, List, Optional, Tuple, Union
import re

# Third Party
from fastapi import Request
from pydantic import ValidationError
from typing_extensions import TypedDict
from vllm.entrypoints.openai.protocol import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ErrorResponse,
)
import orjson

# Local
from vllm_detector_adapter.detector_dispatcher import detector_dispatcher
from vllm_detector_adapter.generative_detectors.base import ChatCompletionDetectionBase
from vllm_detector_adapter.logging import init_logger
from vllm_detector_adapter.protocol import (
    ChatDetectionRequest,
    ContextAnalysisRequest,
    DetectionChatMessageParam,
    DetectionResponse,
    GenerationDetectionRequest,
)
from vllm_detector_adapter.utils import DetectorType

logger = init_logger(__name__)

# OpenAI API provides arguments as a str
class GraniteGuardianToolCallFunctionObject(TypedDict):
    # Name of function to call
    name: str

    # Arguments to call function with, as generated by the model in JSON format
    arguments: Dict


class GraniteGuardian(ChatCompletionDetectionBase):

    DETECTION_TYPE = "risk"
    # User text pattern in task template
    USER_TEXT_PATTERN = "user_text"

    # Model specific tokens
    SAFE_TOKEN = "No"
    UNSAFE_TOKEN = "Yes"

    # Risks associated with context analysis
    PROMPT_CONTEXT_ANALYSIS_RISKS = ["context_relevance"]
    RESPONSE_CONTEXT_ANALYSIS_RISKS = ["groundedness"]

    # Risks associated with generation analysis
    DEFAULT_GENERATION_DETECTION_RISK = "answer_relevance"

    # Risk(s) associated with tools
    TOOLS_RISKS = ["function_call"]
    # Indent level needed for Granite Guardian analysis
    INDENT = orjson.OPT_INDENT_2

    # Risk Bank name defined in the chat template
    RISK_BANK_VAR_NAME = "risk_bank"

    # Attributes to be put in metadata
    METADATA_ATTRIBUTES = ["confidence"]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    ##### Private / Internal functions ###################################################

    def __preprocess(
        self,
        request: Union[
            ChatDetectionRequest, ContextAnalysisRequest, GenerationDetectionRequest
        ],
    ) -> Union[
        ChatDetectionRequest,
        ContextAnalysisRequest,
        GenerationDetectionRequest,
        ErrorResponse,
    ]:
        """Granite guardian specific parameter updates for risk name and risk definition"""
        # Validation that one of the 'defined' risks is requested will be
        # done through the chat template on each request. Errors will
        # be propagated for chat completion separately
        guardian_config = {}
        if not request.detector_params:
            return request

        if risk_name := request.detector_params.pop("risk_name", None):
            guardian_config["risk_name"] = risk_name
        if risk_definition := request.detector_params.pop("risk_definition", None):
            guardian_config["risk_definition"] = risk_definition
        if guardian_config:
            logger.debug("guardian_config {} provided for request", guardian_config)
            # Move the risk name and/or risk definition to chat_template_kwargs
            # to be propagated to tokenizer.apply_chat_template during
            # chat completion
            if "chat_template_kwargs" in request.detector_params:
                # Avoid overwriting other existent chat_template_kwargs
                request.detector_params["chat_template_kwargs"][
                    "guardian_config"
                ] = guardian_config
            else:
                request.detector_params["chat_template_kwargs"] = {
                    "guardian_config": guardian_config
                }

        return request

    def _make_tools_request(
        self, request: ChatDetectionRequest
    ) -> Union[ChatDetectionRequest, ErrorResponse]:
        """Convert original chat detection request to Granite Guardian-compatible request with
        assistant, user, and tools messages.

        This function focuses on processing each of the 3 types of messages, which are also
        enumerated below in the comments:
        (1) Assistant message - contains function calls (function(s) in tool_calls generally
        from an LLM response. These are to be analyzed for risk based on the tool definitions
        in the tools message and the user query. Functions here are referred to as tool_call_function(s)
        (2) User message - a user query, generally passed through as is
        (3) Tools message - contains tools definitions, as a list of functions. Functions here
        are referred to as tool_function(s)
        """

        if (
            "risk_name" not in request.detector_params
            or request.detector_params["risk_name"] not in self.TOOLS_RISKS
        ):
            # Provide error here, since otherwise follow-on tools message
            # and assistant message flattening will not be applicable
            return ErrorResponse(
                message="tools analysis is not supported with given risk",
                type="BadRequestError",
                code=HTTPStatus.BAD_REQUEST.value,
            )

        # (1) 'Flatten' the assistant message, extracting the functions in the tool_calls
        # portion of the message
        assistant_message_count = 0
        assistant_message = None
        user_message_count = 0
        user_message = None
        # NOTE: Guardian models expect json to be serialized and indented
        for i, message in enumerate(request.messages):
            assistant_tool_call_functions = []
            if (
                message["role"] == "assistant"
                and "tool_calls" in message
                and message["tool_calls"]
            ):
                for tool_call in message["tool_calls"]:
                    tool_call_function = tool_call["function"]
                    # OpenAI stores function arguments as a str, but
                    # Granite Guardian expects the arguments as a Dict.
                    # We still convert so that there is no unexpected behavior
                    # when all the functions are converted to json string
                    # with indentation
                    arg_dict = orjson.loads(tool_call_function["arguments"])
                    new_tool_call_function = GraniteGuardianToolCallFunctionObject(
                        name=tool_call_function["name"], arguments=arg_dict
                    )
                    assistant_tool_call_functions.append(new_tool_call_function)
                assistant_content_string = orjson.dumps(
                    assistant_tool_call_functions, option=self.INDENT
                ).decode()
                assistant_message = DetectionChatMessageParam(
                    role=message["role"], content=assistant_content_string
                )
                assistant_message_count += 1

            # (2) User messages are just passed through
            if message["role"] == "user":
                user_message = message
                user_message_count += 1

        # Error if no assistant message was found
        if not assistant_message or not assistant_message["content"]:
            return ErrorResponse(
                message="no assistant message was provided with tool_calls for analysis",
                type="BadRequestError",
                code=HTTPStatus.BAD_REQUEST.value,
            )
        # Error if no user message was found
        if not user_message or not user_message["content"]:
            return ErrorResponse(
                message="no user message was provided with content for analysis",
                type="BadRequestError",
                code=HTTPStatus.BAD_REQUEST.value,
            )
        # Warning if multiple assistant messages were found
        if assistant_message_count > 1:
            logger.warning(
                "More than one assistant message with tool_calls was provided. Only the last will be used for analysis with tools."
            )
        # Warning if multiple user messages were found
        if user_message_count > 1:
            logger.warning(
                "More than one user message was provided. Only the last will be used for analysis with tools."
            )

        # 'tools' ref https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools
        # are provided on the same level as 'messages' on the user request to this
        # adapter. 'tools' are generally a list of tools the model may call. In the Granite
        # Guardian context, these are the definition of tools to be used for analysis.
        # (3) Provide the inner tools functions as a message with role: tools
        tools_functions = []
        for tools in request.tools:
            tools_functions.append(tools["function"])
        tools_content_string = orjson.dumps(
            tools_functions, option=self.INDENT
        ).decode()
        tools_message = DetectionChatMessageParam(
            role="tools", content=tools_content_string
        )

        # Replace request portions
        # We do not pass on `tools` to the chat completions request, since they are
        # not meant to be used as part of the chat completions request and could affect
        # request processing behavior. Currently the common code called after this
        # function already does not pass on `tools`, but in case logic changes there,
        # this function already ensures that the `tools` are removed.
        request.tools = []
        # `tools` and `user` messages are more interchangeable order-wise for the request
        # to the model, but the assistant message needs to be last.
        request.messages = [tools_message, user_message, assistant_message]

        return request

    # Decorating this function to make it cleaner for future iterations of this function
    # to support other types of detectors
    @detector_dispatcher(types=[DetectorType.TEXT_CONTEXT_DOC])
    def _request_to_chat_completion_request(
        self, request: ContextAnalysisRequest, model_name: str
    ) -> Union[ChatCompletionRequest, ErrorResponse]:
        NO_RISK_NAME_MESSAGE = "No risk_name for context analysis"

        risk_name = None
        if (
            "chat_template_kwargs" not in request.detector_params
            or "guardian_config" not in request.detector_params["chat_template_kwargs"]
        ):
            return ErrorResponse(
                message=NO_RISK_NAME_MESSAGE,
                type="BadRequestError",
                code=HTTPStatus.BAD_REQUEST.value,
            )
        # Use risk name to determine message format
        if guardian_config := request.detector_params["chat_template_kwargs"][
            "guardian_config"
        ]:
            risk_name = guardian_config["risk_name"]
        else:
            # Leaving off risk name can lead to model/template errors
            return ErrorResponse(
                message=NO_RISK_NAME_MESSAGE,
                type="BadRequestError",
                code=HTTPStatus.BAD_REQUEST.value,
            )

        if len(request.context) > 1:
            # The detector API for context docs detection supports more than one context text
            # but currently chat completions will only take one context. Here, we concatenate
            # multiple contexts together if provided. Models will error if the user request
            # exceeds the model's context length
            logger.warning("More than one context provided. Concatenating contexts.")
        context_text = " ".join(request.context)  # Will not affect single context case
        content = request.content
        # The "context" role is not an officially supported OpenAI role, so this is specific
        # to Granite Guardian. Messages must also be in precise ordering, or model/template
        # errors may occur.
        if risk_name in self.RESPONSE_CONTEXT_ANALYSIS_RISKS:
            # Response analysis
            messages = [
                {"role": "context", "content": context_text},
                {"role": "assistant", "content": content},
            ]
        elif risk_name in self.PROMPT_CONTEXT_ANALYSIS_RISKS:
            # Prompt analysis
            messages = [
                {"role": "context", "content": context_text},
                {"role": "user", "content": content},
            ]
        else:
            # Return error if risk names are not expected ones
            return ErrorResponse(
                message="risk_name {} is not compatible with context analysis".format(
                    risk_name
                ),
                type="BadRequestError",
                code=HTTPStatus.BAD_REQUEST.value,
            )

        # Try to pass all detector_params through as additional parameters to chat completions
        # without additional validation or parameter changes, similar to ChatDetectionRequest processing
        try:
            return ChatCompletionRequest(
                messages=messages,
                model=model_name,
                **request.detector_params,
            )
        except ValidationError as e:
            return ErrorResponse(
                message=repr(e.errors()[0]),
                type="BadRequestError",
                code=HTTPStatus.BAD_REQUEST.value,
            )

    def _extract_metadata(
        self, response: ChatCompletionResponse, choice_index: int, content
    ):
        """Extract metadata from content and update content as necessary"""
        # Avoid messing up metadata order in case content is not present
        metadata = {}
        if content and isinstance(content, str):
            for metadata_attribute in self.METADATA_ATTRIBUTES:
                regex_str = f"<{metadata_attribute}> (.*?) </{metadata_attribute}>"
                # Some (older) Granite Guardian versions may not contain extra information
                # for metadata. Make sure this does not break anything
                if metadata_search := re.search(regex_str, content):
                    # Update choice content as necessary, removing the metadata portion
                    response.choices[choice_index].message.content = re.sub(
                        regex_str, "", content
                    ).strip()
                    metadata_content = metadata_search.group(1).strip()
                    metadata[metadata_attribute] = metadata_content
        return metadata

    ##### General overriding request / response processing functions ##################

    @detector_dispatcher(types=[DetectorType.TEXT_CONTENT])
    def preprocess_request(self, *args, **kwargs):
        # FIXME: This function declaration is temporary and should be removed once we fix following
        # issue with decorator:
        # ISSUE: Because of inheritance, the base class function with same name gets overriden by the function
        # declared below for preprocessing TEXT_CHAT type detectors. This fails the validation inside
        # the detector_dispatcher decorator.
        return super().preprocess_request(
            *args, **kwargs, fn_type=DetectorType.TEXT_CONTENT
        )

    # Used detector_dispatcher decorator to allow for the same function to be called
    # for different types of detectors with different request types etc.
    @detector_dispatcher(types=[DetectorType.TEXT_CHAT])
    def preprocess_request(  # noqa: F811
        self, request: ChatDetectionRequest
    ) -> Union[ChatDetectionRequest, ErrorResponse]:
        """Granite guardian chat request preprocessing"""

        if request.tools:
            # Form tools message and other messages if tools are provided
            request = self._make_tools_request(request)
            if isinstance(request, ErrorResponse):
                return request

        # Detector param updates
        return self.__preprocess(request)

    async def post_process_completion_results(
        self, response: ChatCompletionResponse, scores: List[float], detection_type: str
    ) -> Tuple[ChatCompletionResponse, List[float], str, Optional[List[Dict]]]:
        """Process Granite Guardian chat completion tags for metadata. Metadata corresponds to
        one Dict per choice in the chat completion response. This implementation
        does not require async, but the base impl is async
        """
        metadata_list = []
        for i, choice in enumerate(response.choices):
            content = choice.message.content
            metadata = self._extract_metadata(
                response, i, content
            )  # response could be updated
            metadata_list.append(metadata)
        # Scores and detection type are just passed through
        return response, scores, detection_type, metadata_list

    ##### Overriding model-class specific endpoint functionality ##################

    async def context_analyze(
        self,
        request: ContextAnalysisRequest,
        raw_request: Optional[Request] = None,
    ) -> Union[DetectionResponse, ErrorResponse]:
        """Function used to call chat detection and provide a /context/doc response"""
        # Fetch model name from super class: OpenAIServing
        model_name = self.models.base_model_paths[0].name

        # Task template not applied for context analysis at this time
        # Make model-dependent adjustments for the request
        request = self.__preprocess(request)

        # Since particular chat messages are dependent on Granite Guardian risk definitions,
        # the processing is done here rather than in a separate, general to_chat_completion_request
        # for all context analysis requests.
        chat_completion_request = self._request_to_chat_completion_request(
            request, model_name, fn_type=DetectorType.TEXT_CONTEXT_DOC
        )
        if isinstance(chat_completion_request, ErrorResponse):
            # Propagate any request problems
            return chat_completion_request

        # Calling chat completion and processing of scores is currently
        # the same as for the /chat case
        result = await self.process_chat_completion_with_scores(
            chat_completion_request, raw_request
        )

        if isinstance(result, ErrorResponse):
            # Propagate any errors from OpenAI API
            return result
        else:
            (
                chat_response,
                scores,
                detection_type,
                metadata,
            ) = await self.post_process_completion_results(*result)

        return DetectionResponse.from_chat_completion_response(
            chat_response,
            scores,
            detection_type,
            metadata_per_choice=metadata,
        )

    async def generation_analyze(
        self,
        request: GenerationDetectionRequest,
        raw_request: Optional[Request] = None,
    ) -> Union[DetectionResponse, ErrorResponse]:
        """Function used to call chat detection and provide a /generation response."""

        # Fetch model name from super class: OpenAIServing
        model_name = self.models.base_model_paths[0].name

        # If risk_name is not specifically provided for this endpoint, we will add a
        # risk_name, since the user has already decided to use this particular endpoint
        if "risk_name" not in request.detector_params:
            request.detector_params[
                "risk_name"
            ] = self.DEFAULT_GENERATION_DETECTION_RISK

        # Task template not applied for generation analysis at this time
        # Make model-dependent adjustments for the request
        request = self.__preprocess(request)

        chat_completion_request = request.to_chat_completion_request(model_name)
        if isinstance(chat_completion_request, ErrorResponse):
            # Propagate any request problems
            return chat_completion_request

        result = await self.process_chat_completion_with_scores(
            chat_completion_request, raw_request
        )

        if isinstance(result, ErrorResponse):
            # Propagate any errors from OpenAI API
            return result
        else:
            (
                chat_response,
                scores,
                detection_type,
                metadata,
            ) = await self.post_process_completion_results(*result)

        return DetectionResponse.from_chat_completion_response(
            chat_response,
            scores,
            detection_type,
            metadata_per_choice=metadata,
        )
